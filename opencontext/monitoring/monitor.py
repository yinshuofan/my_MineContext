# -*- coding: utf-8 -*-

# Copyright (c) 2025 Beijing Volcano Engine Technology Co., Ltd.
# SPDX-License-Identifier: Apache-2.0

"""
System Monitor - Collects and manages various system metrics
"""

import json
import threading
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional

from opencontext.models.enums import ContextType
from opencontext.storage.global_storage import get_storage
from opencontext.utils.logging_utils import get_logger

logger = get_logger(__name__)


@dataclass
class TokenUsage:
    """Token usage statistics"""

    model: str
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class ProcessingMetrics:
    """Processing performance metrics"""

    processor_name: str
    operation: str
    duration_ms: int
    context_type: Optional[str] = None
    context_count: int = 1
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class RetrievalMetrics:
    """Retrieval performance metrics"""

    operation: str
    duration_ms: int
    snippets_count: int = 0
    query: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class ContextTypeStats:
    """Context type statistics"""

    context_type: str
    count: int
    last_update: datetime = field(default_factory=datetime.now)


@dataclass
class ProcessingError:
    """Processing error record"""

    error_message: str
    processor_name: str = ""
    context_count: int = 0
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class SchedulerMetrics:
    """Scheduler task execution metrics"""

    task_type: str
    user_key: str
    success: bool
    duration_ms: int
    trigger_mode: str
    error_message: str = ""
    timestamp: datetime = field(default_factory=datetime.now)


class Monitor:
    """System Monitor"""

    def __init__(self):
        self._lock = threading.RLock()

        # Token usage history (keep last 1000 records)
        self._token_usage_history: deque = deque(maxlen=1000)
        self._token_usage_by_model: Dict[str, List[TokenUsage]] = defaultdict(list)

        # Processing performance history (keep last 1000 records)
        self._processing_history: deque = deque(maxlen=1000)
        self._processing_by_type: Dict[str, List[ProcessingMetrics]] = defaultdict(list)

        # Retrieval performance history (keep last 1000 records)
        self._retrieval_history: deque = deque(maxlen=1000)

        # Context type statistics cache
        self._context_type_stats: Dict[str, ContextTypeStats] = {}
        self._stats_cache_ttl = 60  # Cache for 60 seconds
        self._last_stats_update = datetime.min

        # Processing error records (keep last 50 records)
        self._processing_errors: deque = deque(maxlen=50)

        # Scheduler execution history (keep last 1000 records)
        self._scheduler_history: deque = deque(maxlen=1000)

        # Start time
        self._start_time = datetime.now()

        # Auto cleanup old monitoring data on startup
        self._cleanup_old_data()

        logger.info("System monitor initialized")

    def _cleanup_old_data(self):
        """Clean up monitoring data older than 7 days"""

        try:
            get_storage().cleanup_old_monitoring_data(days=7)
        except Exception as e:
            logger.error(f"Failed to cleanup old monitoring data: {e}")

    def record_token_usage(
        self, model: str, prompt_tokens: int = 0, completion_tokens: int = 0, total_tokens: int = 0
    ):
        """Record token usage"""
        with self._lock:
            usage = TokenUsage(
                model=model,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
            )
            self._token_usage_history.append(usage)
            self._token_usage_by_model[model].append(usage)

            # Limit history size per model
            if len(self._token_usage_by_model[model]) > 100:
                self._token_usage_by_model[model] = self._token_usage_by_model[model][-100:]

            # Persist to database
            self._persist_token_usage(model, prompt_tokens, completion_tokens, total_tokens)

    def _persist_token_usage(
        self, model: str, prompt_tokens: int, completion_tokens: int, total_tokens: int
    ):
        """Persist token usage to database"""
        try:
            get_storage().save_monitoring_token_usage(
                model, prompt_tokens, completion_tokens, total_tokens
            )
        except Exception as e:
            logger.error(f"Failed to persist token usage: {e}")

    def record_processing_metrics(
        self,
        processor_name: str,
        operation: str,
        duration_ms: int,
        context_type: Optional[str] = None,
        context_count: int = 1,
    ):
        """Record processing performance metrics"""
        with self._lock:
            metrics = ProcessingMetrics(
                processor_name=processor_name,
                operation=operation,
                duration_ms=duration_ms,
                context_type=context_type,
                context_count=context_count,
            )
            self._processing_history.append(metrics)
            key = f"{processor_name}:{operation}"
            self._processing_by_type[key].append(metrics)

            # Limit history size
            if len(self._processing_by_type[key]) > 100:
                self._processing_by_type[key] = self._processing_by_type[key][-100:]

    def record_retrieval_metrics(
        self, operation: str, duration_ms: int, snippets_count: int = 0, query: Optional[str] = None
    ):
        """Record retrieval performance metrics"""
        with self._lock:
            metrics = RetrievalMetrics(
                operation=operation,
                duration_ms=duration_ms,
                snippets_count=snippets_count,
                query=query,
            )
            self._retrieval_history.append(metrics)

    def get_context_type_stats(self, force_refresh: bool = False) -> Dict[str, int]:
        """Get record count for each context_type"""
        now = datetime.now()

        # Check if cache is expired
        if not force_refresh and now - self._last_stats_update < timedelta(
            seconds=self._stats_cache_ttl
        ):
            return {k: v.count for k, v in self._context_type_stats.items()}

        # Fetch latest statistics from storage
        try:
            with self._lock:
                stats = get_storage().get_all_processed_context_counts()
                stats = {}
                for context_type in ContextType:
                    count = get_storage().get_processed_context_count(context_type.value)
                    stats[context_type.value] = count
            # Update cache
            for context_type_value, count in stats.items():
                self._context_type_stats[context_type_value] = ContextTypeStats(
                    context_type=context_type_value, count=count
                )

            self._last_stats_update = now
            logger.debug(f"Refreshed context_type statistics: {stats}")
            return stats
        except Exception as e:
            logger.error(f"Failed to get context_type statistics: {e}")

        # Return cached data or empty dict
        return {k: v.count for k, v in self._context_type_stats.items()}

    def get_token_usage_summary(self, hours: int = 24) -> Dict[str, Any]:
        """Get token usage summary from database"""
        summary = {
            "total_records": 0,
            "by_model": {},
            "total_tokens": 0,
            "total_prompt_tokens": 0,
            "total_completion_tokens": 0,
        }

        try:
            rows = get_storage().query_monitoring_token_usage(hours)

            model_stats = defaultdict(
                lambda: {"count": 0, "total_tokens": 0, "prompt_tokens": 0, "completion_tokens": 0}
            )

            for row in rows:
                model = row["model"]
                prompt_tokens = row["prompt_tokens"]
                completion_tokens = row["completion_tokens"]
                total_tokens = row["total_tokens"]

                model_stats[model]["count"] += 1
                model_stats[model]["total_tokens"] += total_tokens
                model_stats[model]["prompt_tokens"] += prompt_tokens
                model_stats[model]["completion_tokens"] += completion_tokens

                summary["total_tokens"] += total_tokens
                summary["total_prompt_tokens"] += prompt_tokens
                summary["total_completion_tokens"] += completion_tokens

            summary["by_model"] = dict(model_stats)
            summary["total_records"] = len(rows)

        except Exception as e:
            logger.error(f"Failed to get token usage summary: {e}")

        return summary

    def get_processing_summary(self, hours: int = 24) -> Dict[str, Any]:
        """Get processing performance summary"""
        cutoff_time = datetime.now() - timedelta(hours=hours)

        with self._lock:
            recent_metrics = [m for m in self._processing_history if m.timestamp >= cutoff_time]

            summary = {
                "total_operations": len(recent_metrics),
                "by_processor": {},
                "by_context_type": {},
                "avg_duration_ms": 0,
                "total_contexts_processed": 0,
            }

            if not recent_metrics:
                return summary

            processor_stats = defaultdict(
                lambda: {"count": 0, "total_duration": 0, "avg_duration": 0, "contexts": 0}
            )
            context_stats = defaultdict(
                lambda: {"count": 0, "total_duration": 0, "avg_duration": 0}
            )

            total_duration = 0
            total_contexts = 0

            for metrics in recent_metrics:
                # Stats by processor
                key = f"{metrics.processor_name}:{metrics.operation}"
                processor_stats[key]["count"] += 1
                processor_stats[key]["total_duration"] += metrics.duration_ms
                processor_stats[key]["contexts"] += metrics.context_count

                # Stats by context type
                if metrics.context_type:
                    context_stats[metrics.context_type]["count"] += 1
                    context_stats[metrics.context_type]["total_duration"] += metrics.duration_ms

                total_duration += metrics.duration_ms
                total_contexts += metrics.context_count

            # Calculate averages
            for stats in processor_stats.values():
                if stats["count"] > 0:
                    stats["avg_duration"] = stats["total_duration"] / stats["count"]

            for stats in context_stats.values():
                if stats["count"] > 0:
                    stats["avg_duration"] = stats["total_duration"] / stats["count"]

            summary["by_processor"] = dict(processor_stats)
            summary["by_context_type"] = dict(context_stats)
            summary["avg_duration_ms"] = (
                total_duration / len(recent_metrics) if recent_metrics else 0
            )
            summary["total_contexts_processed"] = total_contexts

            return summary

    def record_processing_stage(
        self,
        stage_name: str,
        duration_ms: int,
        status: str = "success",
        metadata: Optional[str] = None,
    ):
        """Record processing stage timing"""
        try:
            get_storage().save_monitoring_stage_timing(stage_name, duration_ms, status, metadata)
        except Exception as e:
            logger.error(f"Failed to record processing stage: {e}")

    def increment_data_count(
        self,
        data_type: str,
        count: int = 1,
        context_type: Optional[str] = None,
        metadata: Optional[str] = None,
    ):
        """Increment data count"""
        try:
            get_storage().save_monitoring_data_stats(data_type, count, context_type, metadata)
        except Exception as e:
            logger.error(f"Failed to increment data count: {e}")

    def get_stage_timing_summary(self, hours: int = 24) -> Dict[str, Any]:
        """Get stage timing summary from database"""
        summary = {
            "total_operations": 0,
            "by_stage": {},
            "avg_duration_ms": 0,
        }

        try:
            rows = get_storage().query_monitoring_stage_timing(hours)

            stage_stats = defaultdict(
                lambda: {
                    "count": 0,
                    "total_duration": 0,
                    "avg_duration": 0,
                    "success_count": 0,
                    "error_count": 0,
                }
            )

            total_duration = 0

            for row in rows:
                stage_name = row["stage_name"]
                duration_ms = row["duration_ms"]
                status = row["status"]

                stage_stats[stage_name]["count"] += 1
                stage_stats[stage_name]["total_duration"] += duration_ms

                if status == "success":
                    stage_stats[stage_name]["success_count"] += 1
                else:
                    stage_stats[stage_name]["error_count"] += 1

                total_duration += duration_ms

            # Calculate averages
            for stats in stage_stats.values():
                if stats["count"] > 0:
                    stats["avg_duration"] = int(stats["total_duration"] / stats["count"])

            summary["by_stage"] = dict(stage_stats)
            summary["total_operations"] = len(rows)
            summary["avg_duration_ms"] = int(total_duration / len(rows)) if rows else 0

        except Exception as e:
            logger.error(f"Failed to get stage timing summary: {e}")

        return summary

    def get_data_stats_summary(self, hours: int = 24) -> Dict[str, Any]:
        """Get data statistics summary from database"""
        summary = {
            "by_data_type": {},
            "total_data_processed": 0,
            "by_context_type": {},
        }

        try:
            rows = get_storage().query_monitoring_data_stats(hours)

            # Process the grouped data
            for row in rows:
                data_type = row["data_type"]
                count = row["count"]
                context_type = row["context_type"]

                # Aggregate by data type
                if data_type not in summary["by_data_type"]:
                    summary["by_data_type"][data_type] = 0
                summary["by_data_type"][data_type] += count
                summary["total_data_processed"] += count

                # Aggregate context stats (only for 'context' data_type with non-null context_type)
                if data_type == "context" and context_type is not None:
                    if context_type not in summary["by_context_type"]:
                        summary["by_context_type"][context_type] = 0
                    summary["by_context_type"][context_type] += count

        except Exception as e:
            logger.error(f"Failed to get data stats summary: {e}")

        return summary

    def get_data_stats_by_range(self, start_time: datetime, end_time: datetime) -> Dict[str, Any]:
        """Get data statistics by custom time range"""
        summary = {
            "by_data_type": {},
            "total_data_processed": 0,
            "by_context_type": {},
            "time_range": {
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
            },
        }

        try:
            rows = get_storage().query_monitoring_data_stats_by_range(start_time, end_time)

            # Process the grouped data
            for row in rows:
                data_type = row["data_type"]
                count = row["count"]
                context_type = row["context_type"]

                # Aggregate by data type
                if data_type not in summary["by_data_type"]:
                    summary["by_data_type"][data_type] = 0
                summary["by_data_type"][data_type] += count
                summary["total_data_processed"] += count

                # Aggregate context stats (only for 'context' data_type with non-null context_type)
                if data_type == "context" and context_type is not None:
                    if context_type not in summary["by_context_type"]:
                        summary["by_context_type"][context_type] = 0
                    summary["by_context_type"][context_type] += count

        except Exception as e:
            logger.error(f"Failed to get data stats by range: {e}")

        return summary

    def get_data_stats_trend(self, hours: int = 24) -> Dict[str, Any]:
        """Get data statistics trend with time series data"""
        try:
            rows = get_storage().query_monitoring_data_stats_trend(hours)

            # Organize data by data_type for easy frontend consumption
            # Structure: { 'document': [{timestamp, count}, ...], 'context': [...] }
            trend_data = {
                "document": [],
                "context": [],
            }

            # Group by timestamp and data_type
            time_buckets = {}
            for row in rows:
                timestamp = row["timestamp"]
                data_type = row["data_type"]
                count = row["count"]

                if timestamp not in time_buckets:
                    time_buckets[timestamp] = {"document": 0, "context": 0}

                if data_type in time_buckets[timestamp]:
                    time_buckets[timestamp][data_type] += count

            # Convert to sorted time series
            sorted_timestamps = sorted(time_buckets.keys())
            for ts in sorted_timestamps:
                for data_type in ["document", "context"]:
                    trend_data[data_type].append(
                        {"timestamp": ts, "count": time_buckets[ts][data_type]}
                    )

            return {
                "trend": trend_data,
                "timestamps": sorted_timestamps,
            }

        except Exception as e:
            logger.error(f"Failed to get data stats trend: {e}")
            return {"trend": {"document": [], "context": []}, "timestamps": []}

    def record_processing_error(
        self,
        error_message: str,
        processor_name: str = "",
        context_count: int = 0,
        timestamp: datetime = None,
    ):
        """Record processing error"""
        if timestamp is None:
            timestamp = datetime.now()
        with self._lock:
            error = ProcessingError(
                error_message=error_message,
                processor_name=processor_name,
                context_count=context_count,
                timestamp=timestamp,
            )
            self._processing_errors.append(error)

    def get_processing_errors(self, hours: int = 1, top_n: int = 5) -> Dict[str, Any]:
        """Get top N processing errors"""
        cutoff_time = datetime.now() - timedelta(hours=hours)

        with self._lock:
            recent_errors = [e for e in self._processing_errors if e.timestamp >= cutoff_time]

            # Sort by timestamp in descending order and get top N
            recent_errors.sort(key=lambda x: x.timestamp, reverse=True)
            top_errors = recent_errors[:top_n]

            errors_list = [
                {
                    "error_message": e.error_message,
                    "processor_name": e.processor_name,
                    "context_count": e.context_count,
                    "timestamp": e.timestamp.isoformat(),
                }
                for e in top_errors
            ]

            return {
                "errors": errors_list,
                "total_errors": len(recent_errors),
                "time_range_hours": hours,
            }

    def record_scheduler_execution(
        self,
        task_type: str,
        user_key: str,
        success: bool,
        duration_ms: int,
        trigger_mode: str = "user_activity",
        error_message: str = "",
    ):
        """Record scheduler task execution metrics"""
        with self._lock:
            self._scheduler_history.append(
                SchedulerMetrics(
                    task_type=task_type,
                    user_key=user_key,
                    success=success,
                    duration_ms=duration_ms,
                    trigger_mode=trigger_mode,
                    error_message=error_message,
                )
            )
        # Persist to MySQL via existing stage_timing table
        status = "success" if success else "error"
        self.record_processing_stage(
            stage_name=f"scheduler:{task_type}",
            duration_ms=duration_ms,
            status=status,
            metadata=json.dumps({"user_key": user_key, "trigger_mode": trigger_mode}),
        )

    def get_scheduler_summary(self, hours: int = 24) -> Dict[str, Any]:
        """Get scheduler execution summary"""
        cutoff_time = datetime.now() - timedelta(hours=hours)

        with self._lock:
            recent_metrics = [m for m in self._scheduler_history if m.timestamp >= cutoff_time]

            summary: Dict[str, Any] = {
                "total_executions": len(recent_metrics),
                "by_task_type": {},
                "by_trigger_mode": {},
                "recent_failures": [],
            }

            if not recent_metrics:
                return summary

            task_type_stats: Dict[str, Dict[str, Any]] = defaultdict(
                lambda: {
                    "count": 0,
                    "success_count": 0,
                    "failure_count": 0,
                    "total_duration_ms": 0,
                    "avg_duration_ms": 0,
                    "max_duration_ms": 0,
                    "min_duration_ms": float("inf"),
                }
            )
            trigger_mode_stats: Dict[str, Dict[str, int]] = defaultdict(
                lambda: {"count": 0, "success_count": 0, "failure_count": 0}
            )

            failures = []

            for m in recent_metrics:
                # Stats by task type
                ts = task_type_stats[m.task_type]
                ts["count"] += 1
                ts["total_duration_ms"] += m.duration_ms
                if m.duration_ms > ts["max_duration_ms"]:
                    ts["max_duration_ms"] = m.duration_ms
                if m.duration_ms < ts["min_duration_ms"]:
                    ts["min_duration_ms"] = m.duration_ms
                if m.success:
                    ts["success_count"] += 1
                else:
                    ts["failure_count"] += 1

                # Stats by trigger mode
                tm = trigger_mode_stats[m.trigger_mode]
                tm["count"] += 1
                if m.success:
                    tm["success_count"] += 1
                else:
                    tm["failure_count"] += 1

                # Collect failures
                if not m.success:
                    failures.append(m)

            # Calculate averages and fix min for empty types
            for stats in task_type_stats.values():
                if stats["count"] > 0:
                    stats["avg_duration_ms"] = int(stats["total_duration_ms"] / stats["count"])
                if stats["min_duration_ms"] == float("inf"):
                    stats["min_duration_ms"] = 0
                del stats["total_duration_ms"]  # Don't expose internal accumulator

            # Recent failures (last 10, newest first)
            failures.sort(key=lambda x: x.timestamp, reverse=True)
            recent_failures = [
                {
                    "task_type": f.task_type,
                    "user_key": f.user_key,
                    "error_message": f.error_message,
                    "duration_ms": f.duration_ms,
                    "timestamp": f.timestamp.isoformat(),
                }
                for f in failures[:10]
            ]

            summary["by_task_type"] = dict(task_type_stats)
            summary["by_trigger_mode"] = dict(trigger_mode_stats)
            summary["recent_failures"] = recent_failures

            return summary

    def get_retrieval_summary(self, hours: int = 24) -> Dict[str, Any]:
        """Get retrieval performance summary"""
        cutoff_time = datetime.now() - timedelta(hours=hours)

        with self._lock:
            recent_metrics = [m for m in self._retrieval_history if m.timestamp >= cutoff_time]

            summary = {
                "total_operations": len(recent_metrics),
                "by_operation": {},
                "avg_duration_ms": 0,
                "total_snippets": 0,
                "avg_snippets_per_query": 0,
            }

            if not recent_metrics:
                return summary

            operation_stats = defaultdict(
                lambda: {"count": 0, "total_duration": 0, "avg_duration": 0, "snippets": 0}
            )

            total_duration = 0
            total_snippets = 0

            for metrics in recent_metrics:
                operation_stats[metrics.operation]["count"] += 1
                operation_stats[metrics.operation]["total_duration"] += metrics.duration_ms
                operation_stats[metrics.operation]["snippets"] += metrics.snippets_count

                total_duration += metrics.duration_ms
                total_snippets += metrics.snippets_count

            # Calculate averages
            for stats in operation_stats.values():
                if stats["count"] > 0:
                    stats["avg_duration"] = stats["total_duration"] / stats["count"]

            summary["by_operation"] = dict(operation_stats)
            summary["avg_duration_ms"] = (
                total_duration / len(recent_metrics) if recent_metrics else 0
            )
            summary["total_snippets"] = total_snippets
            summary["avg_snippets_per_query"] = (
                total_snippets / len(recent_metrics) if recent_metrics else 0
            )

            return summary

    def get_system_overview(self) -> Dict[str, Any]:
        """Get system overview"""
        uptime = datetime.now() - self._start_time

        return {
            "uptime_seconds": int(uptime.total_seconds()),
            "uptime_formatted": str(uptime).split(".")[0],
            "context_types": self.get_context_type_stats(),
            "token_usage_24h": self.get_token_usage_summary(hours=24),
            "token_usage_7d": self.get_token_usage_summary(hours=168),  # 7 days
            "processing": self.get_processing_summary(hours=24),
            "stage_timing": self.get_stage_timing_summary(hours=24),
            "data_stats_24h": self.get_data_stats_summary(hours=24),
            "scheduler": self.get_scheduler_summary(hours=24),
            "last_updated": datetime.now().isoformat(),
        }


# Global monitor instance
_monitor: Optional[Monitor] = None
_monitor_lock = threading.Lock()


def get_monitor() -> Monitor:
    """Get global monitor instance"""
    global _monitor
    if _monitor is None:
        with _monitor_lock:
            if _monitor is None:
                _monitor = Monitor()
    return _monitor


def initialize_monitor() -> Monitor:
    """Initialize monitor"""
    monitor = get_monitor()
    return monitor


# Convenient global functions for reporting metrics
def record_token_usage(
    model: str, prompt_tokens: int = 0, completion_tokens: int = 0, total_tokens: int = 0
):
    """Global function: Record token usage"""
    get_monitor().record_token_usage(model, prompt_tokens, completion_tokens, total_tokens)


def record_processing_metrics(
    processor_name: str,
    operation: str,
    duration_ms: int,
    context_type: Optional[str] = None,
    context_count: int = 1,
):
    """Global function: Record processing performance metrics"""
    get_monitor().record_processing_metrics(
        processor_name, operation, duration_ms, context_type, context_count
    )


def record_retrieval_metrics(
    operation: str, duration_ms: int, snippets_count: int = 0, query: Optional[str] = None
):
    """Global function: Record retrieval performance metrics"""
    get_monitor().record_retrieval_metrics(operation, duration_ms, snippets_count, query)


def record_processing_error(
    error_message: str, processor_name: str = "", context_count: int = 0, timestamp: datetime = None
):
    """Global function: Record processing error"""
    get_monitor().record_processing_error(error_message, processor_name, context_count, timestamp)


def record_processing_stage(
    stage_name: str, duration_ms: int, status: str = "success", metadata: Optional[str] = None
):
    """Global function: Record processing stage timing"""
    get_monitor().record_processing_stage(stage_name, duration_ms, status, metadata)


def increment_context_count(context_type: str):
    """Global function: Increment context count by type"""
    get_monitor().increment_data_count("context", context_type=context_type)


def increment_data_count(
    data_type: str,
    count: int = 1,
    context_type: Optional[str] = None,
    metadata: Optional[str] = None,
):
    """Global function: Increment data count"""
    get_monitor().increment_data_count(data_type, count, context_type, metadata)


def record_scheduler_execution(
    task_type: str,
    user_key: str,
    success: bool,
    duration_ms: int,
    trigger_mode: str = "user_activity",
    error_message: str = "",
):
    """Global function: Record scheduler task execution metrics"""
    get_monitor().record_scheduler_execution(
        task_type, user_key, success, duration_ms, trigger_mode, error_message
    )
