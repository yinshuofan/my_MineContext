import os
import sys
import json
import asyncio
from typing import List, Dict, Any

from opencontext.storage.global_storage import get_storage
from opencontext.config.global_config import GlobalConfig

# å‡è®¾ä»£ç åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼Œæ·»åŠ è·¯å¾„ä»¥ä¾¿å¯¼å…¥ opencontext
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import openai
from openai.types.chat import ChatCompletionMessageToolCall, ChatCompletionChunk

# å¯¼å…¥ MineContext çš„æ ¸å¿ƒç»„ä»¶
# æ³¨æ„ï¼šä½ éœ€è¦ç¡®ä¿ä¸Šä¸€è½®ç”Ÿæˆçš„ text_chat_capture.py å’Œ text_chat_processor.py å·²ç»å­˜åœ¨
from opencontext.context_capture.text_chat_capture import TextChatCapture
from opencontext.server.component_initializer import ComponentInitializer
from opencontext.managers.capture_manager import ContextCaptureManager
from opencontext.tools.tool_definitions import ALL_TOOL_DEFINITIONS

# å¯¼å…¥å…·ä½“çš„å·¥å…·ç±»ä»¥ä¾¿æ‰§è¡Œï¼ˆè¿™é‡Œä»…åˆ—ä¸¾éƒ¨åˆ†æ ¸å¿ƒå·¥å…·ä½œä¸ºç¤ºä¾‹ï¼‰
# å®é™…è¿è¡Œæ—¶å»ºè®®ä½¿ç”¨ ToolsExecutor æˆ–æ„å»ºä¸€ä¸ªæ˜ å°„è¡¨
from opencontext.tools.retrieval_tools import (
    ActivityContextTool, 
    SemanticContextTool, 
    IntentContextTool,
    GetTodosTool
)

# --- 1. åˆå§‹åŒ– MineContext è®°å¿†æ¨¡å— ---
def init_memory_module():
    # 1. åŠ¡å¿…å…ˆè·å–å…¨å±€é…ç½®å’Œå­˜å‚¨å®ä¾‹
    GlobalConfig.get_instance()
    storage = get_storage()  # <---ã€å…³é”®ä¿®å¤ã€‘ï¼šè¿™é‡Œéœ€è¦è·å–å­˜å‚¨å®ä¾‹
    
    if not storage:
        print("[System] è­¦å‘Šï¼šå­˜å‚¨æ¨¡å—åˆå§‹åŒ–å¤±è´¥ï¼")

    capture_manager = ContextCaptureManager()
    
    # åˆå§‹åŒ–èŠå¤©æ•è·ç»„ä»¶
    chat_capture = TextChatCapture()
    chat_capture.initialize({"buffer_size": 4}) 
    chat_capture.start()
    
    capture_manager.register_component("text_chat", chat_capture)
    
    from opencontext.context_processing.processor.text_chat_processor import TextChatProcessor
    processor = TextChatProcessor()
    
    # å®šä¹‰å¤„ç†å®Œæˆåçš„å›è°ƒ
    def on_processed(contexts):
        # ç°åœ¨è¿™é‡Œçš„ storage å¼•ç”¨çš„æ˜¯ä¸Šé¢è·å–åˆ°çš„å®ä¾‹
        if contexts and storage:
            try:
                # çœŸæ­£å†™å…¥å‘é‡æ•°æ®åº“
                doc_ids = storage.batch_upsert_processed_context(contexts)
                print(f"\n[System] ğŸ§  è®°å¿†æ€»ç»“å®Œæˆï¼Œå·²æŒä¹…åŒ– {len(contexts)} æ¡è®°å½•ã€‚IDs: {doc_ids}")
            except Exception as e:
                print(f"\n[System] âŒ è®°å¿†å­˜å‚¨å¤±è´¥: {e}")
        else:
            print("\n[System] å¤„ç†å®Œæˆï¼Œä½†æ²¡æœ‰å†…å®¹éœ€è¦å­˜å‚¨æˆ–å­˜å‚¨æ¨¡å—æœªå°±ç»ªã€‚")
        
    processor.set_callback(on_processed)
    
    # å°† Capture çš„è¾“å‡ºè¿æ¥åˆ° Processor
    chat_capture.set_callback(lambda ctxs: [processor.process(c) for c in ctxs])
    
    return chat_capture

# --- 2. å·¥å…·æ‰§è¡Œå™¨ (ç®€å•çš„å·¥å…·åˆ†å‘é€»è¾‘) ---
async def execute_tool(tool_call: ChatCompletionMessageToolCall) -> Dict[str, Any]:
    name = tool_call.function.name
    arguments = json.loads(tool_call.function.arguments)
    print(f"\n[Tool] æ­£åœ¨è°ƒç”¨å·¥å…·: {name} å‚æ•°: {arguments}")

    # ç®€å•çš„å·¥å…·æ˜ å°„è¡¨
    tool_map = {
        "retrieve_activity_context": ActivityContextTool,
        "retrieve_semantic_context": SemanticContextTool,
        "retrieve_intent_context": IntentContextTool,
        "get_todos": GetTodosTool,
        # æ·»åŠ æ›´å¤šå·¥å…·...
    }

    if name in tool_map:
        tool_instance = tool_map[name]()
        # å‡è®¾å·¥å…·éƒ½æœ‰ run æ–¹æ³•ï¼Œæ ¹æ® MineContext çš„ BaseTool å®šä¹‰
        # å¤§å¤šæ•°å·¥å…·çš„å‚æ•°æ˜¯ query æˆ– filters
        try:
            result = tool_instance.run(**arguments)
            return {
                "tool_call_id": tool_call.id,
                "role": "tool",
                "name": name,
                "content": str(result)
            }
        except Exception as e:
            return {
                "tool_call_id": tool_call.id,
                "role": "tool",
                "name": name,
                "content": f"Error: {str(e)}"
            }
    else:
        return {
            "tool_call_id": tool_call.id,
            "role": "tool",
            "name": name,
            "content": "Error: Tool not found"
        }

# --- 3. ä¸»èŠå¤©é€»è¾‘ ---
async def chat_loop():
    # é…ç½® LLM
    client = openai.AsyncOpenAI(
        api_key="cd8b23c5-45f1-48a8-9009-e1ba7f592cfe",
        base_url="https://ark.cn-beijing.volces.com/api/v3",
    )
    
    model_name = "doubao-seed-1-6-251015"
    
    # åˆå§‹åŒ–è®°å¿†æ•è·
    chat_capture = init_memory_module()
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ‹¥æœ‰é•¿æœŸè®°å¿†çš„æ™ºèƒ½åŠ©æ‰‹ã€‚ä½ å¯ä»¥ä½¿ç”¨å·¥å…·æ£€ç´¢è¿‡å»çš„å¯¹è¯å’Œæ´»åŠ¨ã€‚"}
    ]

    print("=== MineContext èŠå¤©æœºå™¨äºº (è¾“å…¥ 'quit' é€€å‡º) ===")

    while True:
        user_input = input("\nUser: ")
        if user_input.lower() in ["quit", "exit"]:
            break

        # 1. è®°å½•ç”¨æˆ·æ¶ˆæ¯åˆ°çŸ­æœŸä¸Šä¸‹æ–‡
        messages.append({"role": "user", "content": user_input})
        
        # 2. æ¨é€æ¶ˆæ¯åˆ° MineContext é•¿æœŸè®°å¿†æ•è·æ¨¡å—
        chat_capture.push_message("user", user_input)

        # 3. è¯·æ±‚ LLM
        response = await client.chat.completions.create(
            model=model_name,
            messages=messages,
            stream=True,
            reasoning_effort="minimal", # å¦‚æœæ¨¡å‹æ”¯æŒ
            tools=ALL_TOOL_DEFINITIONS, # æ³¨å…¥ MineContext çš„æ‰€æœ‰å·¥å…·å®šä¹‰
            tool_choice="auto",
        )

        # 4. å¤„ç†æµå¼å“åº”
        print("Assistant: ", end="", flush=True)
        
        collected_content = ""
        tool_calls_buffer = []
        current_tool_call = None

        async for chunk in response:
            delta = chunk.choices[0].delta
            
            # A. å¤„ç†æ–‡æœ¬å†…å®¹
            if delta.content:
                print(delta.content, end="", flush=True)
                collected_content += delta.content
            
            # B. å¤„ç†å·¥å…·è°ƒç”¨ (æµå¼å·¥å…·è°ƒç”¨éœ€è¦æ‹¼æ¥)
            if delta.tool_calls:
                for tc_chunk in delta.tool_calls:
                    if len(tool_calls_buffer) <= tc_chunk.index:
                        tool_calls_buffer.append({
                            "id": "", "type": "function", "function": {"name": "", "arguments": ""}
                        })
                    
                    tc = tool_calls_buffer[tc_chunk.index]
                    if tc_chunk.id: tc["id"] += tc_chunk.id
                    if tc_chunk.function.name: tc["function"]["name"] += tc_chunk.function.name
                    if tc_chunk.function.arguments: tc["function"]["arguments"] += tc_chunk.function.arguments

        # 5. å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ‰§è¡Œå¹¶è¿›è¡Œç¬¬äºŒè½®å¯¹è¯
        if tool_calls_buffer:
            # æ·»åŠ  Assistant çš„ tool_calls æ¶ˆæ¯åˆ°å†å²
            assistant_msg = {
                "role": "assistant",
                "content": collected_content if collected_content else None,
                "tool_calls": tool_calls_buffer
            }
            messages.append(assistant_msg)
            
            # æ‰§è¡Œæ‰€æœ‰å·¥å…·
            for tc_data in tool_calls_buffer:
                # æ„é€ ä¸´æ—¶çš„ ToolCall å¯¹è±¡ä»¥ä¾¿å¤ç”¨ execute_tool å‡½æ•°
                class MockToolCall:
                    id = tc_data["id"]
                    class Function:
                        name = tc_data["function"]["name"]
                        arguments = tc_data["function"]["arguments"]
                    function = Function()
                
                tool_result_msg = await execute_tool(MockToolCall())
                messages.append(tool_result_msg) # æ·»åŠ  Tool ç»“æœæ¶ˆæ¯

            # å¸¦ä¸Šå·¥å…·ç»“æœå†æ¬¡è¯·æ±‚ LLM
            # æ³¨æ„ï¼šè¿™é‡Œä¸éœ€è¦å†è®°å½•ä¸€æ¬¡ push_messageï¼Œå› ä¸ºè¿™å±äºæ€è€ƒè¿‡ç¨‹
            response_2 = await client.chat.completions.create(
                model=model_name,
                messages=messages,
                stream=True,
                tools=ALL_TOOL_DEFINITIONS
            )
            
            # è¾“å‡ºç¬¬äºŒè½®ç»“æœ
            collected_content = "" # é‡ç½®å†…å®¹
            async for chunk in response_2:
                content = chunk.choices[0].delta.content
                if content:
                    print(content, end="", flush=True)
                    collected_content += content

        print() # æ¢è¡Œ

        # 6. è®°å½•åŠ©æ‰‹æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡
        messages.append({"role": "assistant", "content": collected_content})
        
        # 7. æ¨é€åŠ©æ‰‹å›å¤åˆ° MineContext é•¿æœŸè®°å¿†æ•è·æ¨¡å—
        chat_capture.push_message("assistant", collected_content)

if __name__ == "__main__":
    asyncio.run(chat_loop())